
There is a revolution occurring in the data world. Driven by technological advancements, the current wave of open source data formats is changing the game for the entire ecosystem - from vendor to enterprise.

Data needs organization and analysis to generate timely insights to help companies make better decisions and improve operations. Working with unstructured data like images, .pdfs, audio, videos, etc., presents different challenges. Unstructured data like .csv, .xml, .json, etc. are difficult to optimize.

The ability to generate insights from these datasets depends on how the data is organized. Due to the growing complexity of the business data enterprises are recording, they might find that instead of collecting 20 fields for each data event, they are now capturing fields in hundreds. While this data is easy to store in a data lake, querying it will require scanning a significant amount of data if stored in row-based formats.

The ability to query and manipulate data with data processing, artificial intelligence/machine learning (AI/ML), business intelligence and reporting workloads becomes critical. These workloads will invariably need to fetch small data ranges from enormous datasets. Hence the need for data formats.

If you have interacted with any data engineer, you must have heard of different data formats like Parquet, ORC, Avro, Arrow, Protobuf, Thrift, MessagePack, etc . What are all these files and data formats? Where are they applicable? How to choose the right design for the right job? These are some of the questions we will decipher in this article.

## Data Formats for Microservices
Before diving deep into the data formats for efficient storage and retrieval in a data lake, let's look at the layouts not so relevant to the data lake. Data formats like Protobuf, Thrift and MessagePack are more relevant to the intercommunication.




